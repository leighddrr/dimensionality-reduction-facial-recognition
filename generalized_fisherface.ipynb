{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Generalized Fisherface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Fisherface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: OPTIMIZATION SEEMS TO BE SOMEHOW NOT WELL POSED. WHY? NO BOUNDS, LOSS IS ARBITRARILY SMALL / MAXIMIZATION ARBITRARILY LARGE\n",
    "\n",
    "Q: Is fisherace omitting some details about the optimization? what's to stop us from getting arbitrarily low loss through an arbitrarily large matrix? is the optimization supposed to be over matrices with columns with unit length? If so, how can I turn this into an unconstrained optimization problem so that I can use gradient descent or similar optimization algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (via Gradient Decent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myPCA():\n",
    "    '''An implementation of PCA using gradient decent'''\n",
    "    # used for debugging implementation of generalized fisherface...\n",
    "    def __init__(self, X, n_components, optimizer=torch.optim.Adam, regularization_func=torch.linalg.matrix_norm,\n",
    "                    lr=1e-2, n_steps=100, l2_coef=1, stop_thres=1e-3, verbose=True, **optim_kwargs):\n",
    "\n",
    "        self.n_samples = np.shape(X)[0]\n",
    "        self.dim = np.shape(X)[1]\n",
    "        self.n_components = n_components\n",
    "\n",
    "        self.l2_coef = l2_coef\n",
    "        self.regularization_func = regularization_func\n",
    "\n",
    "        self.stop_thres = stop_thres\n",
    "\n",
    "        W = torch.eye(self.dim, self.n_components) # (dim x n_compoenents) matrix\n",
    "        self.W = nn.Parameter(W)\n",
    "\n",
    "        # NOTE: we assume normalization prior to calling this...\n",
    "        global_mean = np.mean(X, axis=0)\n",
    "\n",
    "        # compute total scatter matrix\n",
    "        self.S_T = (X - global_mean).T @ (X - global_mean)\n",
    "        # self.S_T = np.cov(X.T)# NOTE TEMP\n",
    "        self.S_T = torch.tensor(self.S_T, dtype=torch.float32)\n",
    "\n",
    "        self.optimizer = optimizer([self.W], lr=lr, **optim_kwargs)\n",
    "        self.n_steps = n_steps\n",
    "        self.loss_history = []\n",
    "        self.objective_history = []\n",
    "\n",
    "    def base_objective_func(self, W):\n",
    "        return torch.det(W.t() @ self.S_T @ W)\n",
    "\n",
    "    def loss(self):\n",
    "        # (negative since we're maximizing) + L2 regularization to implement constrained optimization (NOTE TEMP experimentating)\n",
    "        loss = - self.base_objective_func(self.W) + self.l2_coef * self.regularization_func(self.W)\n",
    "        return loss\n",
    "\n",
    "    def normalized_W(self):\n",
    "        return self.W / torch.sqrt(torch.sum(self.W**2, axis=0))\n",
    "\n",
    "    @property\n",
    "    def W_opt(self):\n",
    "        '''returns normalized transformation matrix'''\n",
    "        return self.normalized_W()\n",
    "\n",
    "    def fit(self, X=None, y=None, n_steps=None):\n",
    "        if n_steps is None:\n",
    "            n_steps = self.n_steps\n",
    "\n",
    "        print('Fitting transformation...')\n",
    "        obj_prev = self.base_objective_func(self.W_opt).detach().numpy()\n",
    "        for _ in tqdm(range(n_steps)):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.loss_history.append(loss.detach().numpy())\n",
    "\n",
    "            objective = self.base_objective_func(self.W_opt).detach().numpy()\n",
    "            self.objective_history.append(objective)\n",
    "\n",
    "            if self.stop_thres is not None:\n",
    "                if np.max(np.abs(obj_prev - objective)) < self.stop_thres:\n",
    "                    print('Early stopping.')\n",
    "                    break\n",
    "\n",
    "                obj_prev = self.base_objective_func(self.W_opt).detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.W_opt.t() @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "dim = 64*64\n",
    "n_classes = 6\n",
    "n_components = 5\n",
    "\n",
    "# X1 = np.random.uniform(size=n_samples)\n",
    "# X2 = 2*X1 + 0.2*np.random.normal(size=n_samples)\n",
    "# X = np.array((X1, X2)).T\n",
    "\n",
    "X = np.random.uniform(size=(n_samples, dim))\n",
    "y = np.random.randint(0, n_classes-1, size=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4096)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = myPCA(X, n_components, optimizer=torch.optim.Adam, lr=1e-2, l2_coef=2, regularization_func=lambda x: torch.linalg.matrix_norm(x, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 98/1000 [00:06<00:56, 15.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pca.fit(n_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0895, -0.0156,  0.0115,  0.0122,  0.0144],\n",
       "        [-0.0130,  0.0907, -0.0132,  0.0249, -0.0137],\n",
       "        [ 0.0190, -0.0156,  0.0907,  0.0376, -0.0180],\n",
       "        ...,\n",
       "        [ 0.0112, -0.0131, -0.0147, -0.0125,  0.0137],\n",
       "        [ 0.0137, -0.0148,  0.0131,  0.0236,  0.0238],\n",
       "        [-0.0145, -0.0140,  0.0125, -0.0117, -0.0137]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.W_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19582231400>]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.objective_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7428, 0.7430, 0.7375, 0.7491],\n",
       "        [0.7428, 1.0000, 0.7493, 0.7447, 0.7451],\n",
       "        [0.7430, 0.7493, 1.0000, 0.7487, 0.7469],\n",
       "        [0.7375, 0.7447, 0.7487, 1.0000, 0.7439],\n",
       "        [0.7491, 0.7451, 0.7469, 0.7439, 1.0000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.W_opt.t() @ pca.W_opt # W_opt is indeed orthonormal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = PCA(n_components=n_components).fit(X).components_.T # get the transformation obtained by sklearn's PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01384515  0.00303829  0.00909934 -0.01460447  0.00030356]\n",
      " [ 0.00841382 -0.01088396  0.00582504 -0.00083669  0.00919841]\n",
      " [ 0.01105549  0.00057353 -0.00415466  0.00196091  0.02031142]\n",
      " ...\n",
      " [ 0.02514635 -0.01860873 -0.02597897 -0.00851558 -0.00148205]\n",
      " [ 0.00954915 -0.00758734  0.01445983  0.01061876 -0.01338196]\n",
      " [ 0.01323342  0.00599086 -0.00043872  0.01750647  0.0282344 ]]\n"
     ]
    }
   ],
   "source": [
    "print(W) # but is different from the transformation obtained via PCA; but it might be 'equivalent'? perhaps same 'subspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8784e+14)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.base_objective_func(torch.tensor(W, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(69316736., grad_fn=<DetLuBasedHelperBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.base_objective_func(pca.W_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results from sklearn's PCA are different. but sklearn's PCA is different each time you run it. how can we validate that the results are 'equivalent'?? compare subspaces?? compare loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the loss obtained by sklearn's PCA implementation is consistently the same even though the `components_` differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my implementation doesn't achieve as high a maximization as sklearn's PCA... why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes the `objective_loss` goes up then back down => the loss function doesn't maximize the objective over orthonormal matrices... the objective is highly dependent on the initialization (if i initialize with identity the objective achieved is different than if randomly initialized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually, for a simple example, my implementation achieaves the exact same loss... is there some kind of instability going on? convergence is achieved for simple examples but for more complicated ones? confirm: this is a convex optimization problem right? should i be looking into optimization algorithms other than gradient decent-based ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisherface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fisherface():\n",
    "    def __init__(self, X, y, n_components=None, pca_first=True, verbose=True, S_B=None, S_W=None):\n",
    "\n",
    "        self.n_samples = np.shape(X)[0]\n",
    "        self.dim = np.shape(X)[1]\n",
    "\n",
    "        self.n_classes = len(np.unique(y))\n",
    "\n",
    "\n",
    "        # set default number components to be number of classes - 1 (if no other number given)\n",
    "        if n_components is None:\n",
    "            # this is the maximum number of components s.t. S_W is non-singular\n",
    "            self.n_components = self.n_classes - 1\n",
    "        else:\n",
    "            self.n_components = n_components\n",
    "\n",
    "        if pca_first:\n",
    "            # perform PCA first to address S_W, the within-class scatter matrix, being singular\n",
    "\n",
    "            # the PCA matrix\n",
    "            if verbose:\n",
    "                print('Performing PCA for initial dimensionality reduction')\n",
    "            pca = PCA(n_components=self.n_samples - self.n_classes).fit(X)\n",
    "            self.W_pca = torch.tensor(pca.components_.T, dtype=torch.float32) # (dim x n_samples - n_classes) matrix\n",
    "            if verbose:\n",
    "                print('Done.')\n",
    "\n",
    "            # the fisher's linear discriminant matrix\n",
    "            # TODO: consider what the best choice for intialization would be.\n",
    "            W_fld = torch.eye(self.n_samples - self.n_classes, self.n_components) \\\n",
    "                    + torch.rand(self.n_samples - self.n_classes, self.n_components) # (n_samples - n_classes x n_compoenents) matrix\n",
    "            self.W_fld = nn.Parameter(W_fld)\n",
    "\n",
    "        else:\n",
    "            # don't perform PCA, perform standard fisher's linear discriminant\n",
    "            self.W_pca = torch.eye(self.dim)\n",
    "\n",
    "            W_fld = torch.Tensor(self.n_samples - self.n_classes, self.n_components) # (n_samples - n_classes x n_compoenents) matrix\n",
    "            self.W_fld = nn.Parameter(W_fld)\n",
    "\n",
    "\n",
    "        # NOTE: we assume normalization prior to calling this...\n",
    "        classes = np.sort(np.unique(y)) # classes in the dataset\n",
    "        points_by_class = [X[y==class_] for class_ in classes] # the sets X_i\n",
    "        n_points_by_class = [len(X[y==class_]) for class_ in classes]\n",
    "        class_means = [np.mean(class_points, axis=0) for class_points in points_by_class] # mu_i's\n",
    "        global_mean = np.expand_dims(np.mean(X, axis=0),-1)\n",
    "\n",
    "        # between-class scatter matrix\n",
    "        if verbose:\n",
    "            print('Computing the between-class and within-class scatter matrices...')\n",
    "\n",
    "        # allow for precomputing S_B and S_W for faster debugging\n",
    "        if S_B is None or S_W is None:\n",
    "            self.S_B = np.sum([n_points_by_class[class_] * (class_means[class_] - global_mean).T @ (class_means[class_] - global_mean) for class_ in classes], axis=0)\n",
    "            self.S_W = np.sum([(points_by_class[class_] - class_means[class_]).T @ (points_by_class[class_] - class_means[class_]) for class_ in classes], axis=0)\n",
    "        else:\n",
    "            self.S_B = S_B\n",
    "            self.S_W = S_W\n",
    "\n",
    "        self.S_B = torch.tensor(self.S_B, dtype=torch.float32)\n",
    "        self.S_W = torch.tensor(self.S_W, dtype=torch.float32)\n",
    "        if verbose:\n",
    "            print('Done.')\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        numerator = torch.det(self.W_fld.t() @ self.W_pca.t() @ self.S_B @ self.W_pca @ self.W_fld)\n",
    "        denominator = torch.det(self.W_fld.t() @ self.W_pca.t() @ self.S_W @ self.W_pca @ self.W_fld)\n",
    "        loss = - numerator / denominator # (negative since we're maximizing)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def W_opt(self):\n",
    "        return self.W_pca @ self.W_fld\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_B = fld.S_B.detach().numpy()\n",
    "S_W = fld.S_W.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "dim = 64*64\n",
    "n_classes = 6\n",
    "n_components = 5\n",
    "\n",
    "X = np.random.uniform(size=(n_samples, dim))\n",
    "y = np.random.randint(0, n_classes-1, size=n_samples)\n",
    "\n",
    "# fld = Fisherface(X, y, n_components=n_components, pca_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA for initial dimensionality reduction\n",
      "Done.\n",
      "Computing the between-class and within-class scatter matrices...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "fld = Fisherface(X, y, n_components=n_components, pca_first=True, S_B=S_B, S_W=S_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 61853.2500, -52495.2578, -39789.2031,  13971.1719, -21186.1426],\n",
       "        [ -5974.6821,   8014.5391,  10462.8730,   2579.0161,  -3760.1821],\n",
       "        [ 10566.6855, -35335.0898, -19384.4062,   2742.2375,   4167.3550],\n",
       "        ...,\n",
       "        [-11012.6816,   7640.9126,  13530.7695,  -8384.8271,   6211.9663],\n",
       "        [ 14166.7910, -30040.1055, -10209.0518,   2564.4790,  -1195.6466],\n",
       "        [ 24937.6035, -18554.4766, -13991.1953,  -3230.4907,  -7145.3887]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = fld.loss()\n",
    "loss.backward()\n",
    "fld.W_fld.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.2030, 0.1466, 0.0552, 0.3083, 0.6972],\n",
       "        [0.6202, 1.8754, 0.2999, 0.8033, 0.2063],\n",
       "        [0.7881, 0.0266, 1.0428, 0.4321, 0.4302],\n",
       "        ...,\n",
       "        [0.6960, 0.2532, 0.1509, 0.1046, 0.1454],\n",
       "        [0.8285, 0.1132, 0.5184, 0.6016, 0.3878],\n",
       "        [0.7546, 0.6600, 0.0294, 0.3034, 0.6303]], requires_grad=True)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fld.W_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.78it/s]\n"
     ]
    }
   ],
   "source": [
    "n_steps = int(1e2)\n",
    "optimizer = torch.optim.SGD([fld.W_fld], lr=1e-10)\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(n_steps)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = fld.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-98863.695, dtype=float32),\n",
       " array(-99451.12, dtype=float32),\n",
       " array(-100042.71, dtype=float32),\n",
       " array(-100643.08, dtype=float32),\n",
       " array(-101239.836, dtype=float32),\n",
       " array(-101846.47, dtype=float32),\n",
       " array(-102453.01, dtype=float32),\n",
       " array(-103061.99, dtype=float32),\n",
       " array(-103679., dtype=float32),\n",
       " array(-104297.48, dtype=float32),\n",
       " array(-104920.69, dtype=float32),\n",
       " array(-105559.375, dtype=float32),\n",
       " array(-106183.84, dtype=float32),\n",
       " array(-106816.484, dtype=float32),\n",
       " array(-107460.91, dtype=float32),\n",
       " array(-108106.38, dtype=float32),\n",
       " array(-108752.016, dtype=float32),\n",
       " array(-109408.51, dtype=float32),\n",
       " array(-110066.05, dtype=float32),\n",
       " array(-110721.4, dtype=float32),\n",
       " array(-111395.96, dtype=float32),\n",
       " array(-112067.125, dtype=float32),\n",
       " array(-112738.445, dtype=float32),\n",
       " array(-113422.195, dtype=float32),\n",
       " array(-114103.32, dtype=float32),\n",
       " array(-114793.85, dtype=float32),\n",
       " array(-115487.945, dtype=float32),\n",
       " array(-116189.01, dtype=float32),\n",
       " array(-116891.58, dtype=float32),\n",
       " array(-117595.91, dtype=float32),\n",
       " array(-118308.016, dtype=float32),\n",
       " array(-119024.25, dtype=float32),\n",
       " array(-119748.62, dtype=float32),\n",
       " array(-120478.336, dtype=float32),\n",
       " array(-121201.305, dtype=float32),\n",
       " array(-121949.01, dtype=float32),\n",
       " array(-122689.7, dtype=float32),\n",
       " array(-123438.52, dtype=float32),\n",
       " array(-124185.02, dtype=float32),\n",
       " array(-124940.51, dtype=float32),\n",
       " array(-125706.16, dtype=float32),\n",
       " array(-126469.3, dtype=float32),\n",
       " array(-127244.63, dtype=float32),\n",
       " array(-128027.59, dtype=float32),\n",
       " array(-128807.516, dtype=float32),\n",
       " array(-129603.26, dtype=float32),\n",
       " array(-130395.484, dtype=float32),\n",
       " array(-131195.67, dtype=float32),\n",
       " array(-131994.45, dtype=float32),\n",
       " array(-132804.16, dtype=float32),\n",
       " array(-133621.31, dtype=float32),\n",
       " array(-134452.3, dtype=float32),\n",
       " array(-135274.83, dtype=float32),\n",
       " array(-136110.67, dtype=float32),\n",
       " array(-136956.39, dtype=float32),\n",
       " array(-137805.05, dtype=float32),\n",
       " array(-138641.23, dtype=float32),\n",
       " array(-139502.14, dtype=float32),\n",
       " array(-140369.73, dtype=float32),\n",
       " array(-141235.05, dtype=float32),\n",
       " array(-142112.84, dtype=float32),\n",
       " array(-142998.33, dtype=float32),\n",
       " array(-143880.03, dtype=float32),\n",
       " array(-144781.81, dtype=float32),\n",
       " array(-145684.06, dtype=float32),\n",
       " array(-146577.7, dtype=float32),\n",
       " array(-147494.39, dtype=float32),\n",
       " array(-148417.4, dtype=float32),\n",
       " array(-149348.84, dtype=float32),\n",
       " array(-150269.7, dtype=float32),\n",
       " array(-151210.14, dtype=float32),\n",
       " array(-152163.2, dtype=float32),\n",
       " array(-153113.25, dtype=float32),\n",
       " array(-154066.86, dtype=float32),\n",
       " array(-155035.39, dtype=float32),\n",
       " array(-156008.98, dtype=float32),\n",
       " array(-156986.78, dtype=float32),\n",
       " array(-157968.6, dtype=float32),\n",
       " array(-158978.61, dtype=float32),\n",
       " array(-159976.42, dtype=float32),\n",
       " array(-160986.44, dtype=float32),\n",
       " array(-162002.73, dtype=float32),\n",
       " array(-163015., dtype=float32),\n",
       " array(-164051.77, dtype=float32),\n",
       " array(-165092.61, dtype=float32),\n",
       " array(-166136.36, dtype=float32),\n",
       " array(-167177.52, dtype=float32),\n",
       " array(-168251.8, dtype=float32),\n",
       " array(-169325.34, dtype=float32),\n",
       " array(-170405.69, dtype=float32),\n",
       " array(-171479.14, dtype=float32),\n",
       " array(-172575.77, dtype=float32),\n",
       " array(-173673.62, dtype=float32),\n",
       " array(-174781.6, dtype=float32),\n",
       " array(-175906.42, dtype=float32),\n",
       " array(-177026.53, dtype=float32),\n",
       " array(-178155.61, dtype=float32),\n",
       " array(-179299.3, dtype=float32),\n",
       " array(-180445.77, dtype=float32),\n",
       " array(-181601.3, dtype=float32)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan]], requires_grad=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fld.W_fld"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "205d50f58f82a2b4d9ed38d5fc136ab30afd6ca8c2e73e92b6068ffbf36380d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
